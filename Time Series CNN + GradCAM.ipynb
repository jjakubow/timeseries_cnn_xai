{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with CNN and Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import shap\n",
    "\n",
    "colormap = ListedColormap([\"#ff595e\",\"#ffca3a\",\"#8ac926\",\"#52a675\",\"#1982c4\",\"#6a4c93\"], name=\"Custom\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colormap.colors)\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"jena_climate_2009_2016.csv\"\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "download_and_extract_archive(url=uri, download_root=os.getcwd(), filename=csv_filename+\".zip\")\n",
    "\n",
    "df = pd.read_csv(csv_filename)\n",
    "# convert 'Date Time' to datetime object\n",
    "df['Date Time'] = df['Date Time'].apply(lambda x: datetime.strptime(x, \"%d.%m.%Y %H:%M:%S\"))\n",
    "\n",
    "# convert angle to cos & sin\n",
    "df[\"cos(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.cos(x / 360 * 2 * np.pi))\n",
    "df[\"sin(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.sin(x / 360 * 2 * np.pi))\n",
    "\n",
    "df = df.drop([\"wd (deg)\"], axis=1)\n",
    "\n",
    "features = df.columns.values[1:]\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers\n",
    "\n",
    "remove_outliers = True\n",
    "plot_on_timeseries = True\n",
    "\n",
    "if plot_on_timeseries:\n",
    "    fig, ax = plt.subplots(figsize=(16, 3))\n",
    "\n",
    "for i, X in enumerate(features):\n",
    "    q05 = df[X].quantile(0.05)\n",
    "    q95 = df[X].quantile(0.95)\n",
    "    iqr = df[X].quantile(0.75) - df[X].quantile(0.25)\n",
    "    \n",
    "    lower_bound = q05 - 3 * iqr\n",
    "    upper_bound = q95 + 3 * iqr\n",
    "    \n",
    "    outliers = ((df[X] < lower_bound) | (df[X] > upper_bound))\n",
    "\n",
    "    if outliers.sum() > 0:\n",
    "        print(\"%s has %i outliers (lower=%.4g, upper=%.4g)\" % (X, outliers.sum(), lower_bound, upper_bound))\n",
    "    \n",
    "    if plot_on_timeseries:\n",
    "        label = X\n",
    "        for obs in df.loc[outliers, \"Date Time\"]:\n",
    "            ax.axvline(x=obs, color=colormap(i), label=label)\n",
    "            label=None\n",
    "    \n",
    "    if remove_outliers:\n",
    "        df = df[~outliers]\n",
    "        \n",
    "\n",
    "if plot_on_timeseries:\n",
    "    ax.set_title(\"Outliers\")\n",
    "    ax.legend(bbox_to_anchor=(0.5, 1.2), loc='center', ncol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[features].corr()\n",
    "redundant_features = []\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        col_i = features[i]\n",
    "        col_j = features[j]\n",
    "        \n",
    "        if correlation_matrix.loc[col_i, col_j] > 0.98:\n",
    "            print(\"%s and %s are redundant\" % (col_i, col_j))\n",
    "            redundant_features.append(col_j)\n",
    "\n",
    "df = df.drop(redundant_features, axis=1, errors='ignore')\n",
    "\n",
    "df = df.set_index(\"Date Time\").resample(\"H\").mean().reset_index()\n",
    "\n",
    "features = df.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 4\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    ax.hist(df[X], bins=50)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 2\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "sample_low = 0\n",
    "sample_high = 52560 * 2 # 2 years\n",
    "sample_high = -1\n",
    "skip = 6\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    plot_x = df[\"Date Time\"].iloc[sample_low:sample_high:skip]\n",
    "    plot_y = df[X].iloc[sample_low:sample_high:skip]\n",
    "    ax.plot(plot_x, plot_y, linewidth=1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split, scale & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_future = 12\n",
    "hours_past = 48\n",
    "\n",
    "time_array = df['Date Time']\n",
    "X = df[features]\n",
    "y = df['T (degC)'].shift(-(hours_future + hours_past))\n",
    "\n",
    "# split into test and train datasets\n",
    "time_train, time_test, X_train, X_test, y_train, y_test = train_test_split(time_array, X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# scale datasets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch from pandas Series to numpy array\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "n = 1000\n",
    "ax.plot(np.arange(n), X.iloc[:n, 1], label=\"X_temperature\")\n",
    "ax.plot(np.arange(n), y.iloc[:n], label=\"y\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(X, sequence_length=48):\n",
    "    X_seq = []\n",
    "    for i in range(sequence_length):\n",
    "        X_roll = X.roll(-i, dims=0)\n",
    "        X_seq.append(X_roll)\n",
    "    \n",
    "    X_seq = torch.stack(X_seq, dim=2)\n",
    "    \n",
    "    # expand the dimensions\n",
    "    X_seq = X_seq[:, None, :, :]\n",
    "    \n",
    "    return X_seq\n",
    "\n",
    "\n",
    "def prepare_data(X, y, time_array, hours_past=48, hours_future=12, batch_size=32):\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    \n",
    "    X_ts = prepare_sequences(X, hours_past)\n",
    "    X_ts = X_ts[:-(hours_past + hours_future)]\n",
    "    y_ts = y[:-(hours_past + hours_future)]\n",
    "        \n",
    "    time_array = time_array[:-(hours_past + hours_future)]\n",
    "    \n",
    "    # we verify which observations are valid i.e. there is proper time difference between the observations\n",
    "    # valid_observations = ((time_array.diff(-hours_past).dt.total_seconds() / 3600 ) ==  -hours_past ) & ((time_array.diff(hours_future).dt.total_seconds() / 3600 ) ==  hours_future )\n",
    "    \n",
    "    x_nan_observations = torch.isnan(X_ts).any(dim=1).any(dim=1).any(dim=1)\n",
    "    y_nan_observations = torch.isnan(y_ts)\n",
    "    \n",
    "    nan_observations = x_nan_observations | y_nan_observations\n",
    "    \n",
    "    time_ts = time_array[~nan_observations.numpy()]\n",
    "    X_ts = X_ts[~nan_observations]\n",
    "    y_ts = y_ts[~nan_observations]\n",
    "    \n",
    "    return time_ts, X_ts, y_ts\n",
    "\n",
    "time_ts_train, X_ts_train, y_ts_train = prepare_data(X_train, y_train, time_train)\n",
    "time_ts_test, X_ts_test, y_ts_test = prepare_data(X_test, y_test, time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self,features, target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.features[idx]\n",
    "        label = self.target[idx]\n",
    "        \n",
    "        return item,label\n",
    "    \n",
    "train = WeatherDataset(X_ts_train, y_ts_train)\n",
    "test = WeatherDataset(X_ts_test, y_ts_test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12*48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GradCAM(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 3))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2))\n",
    "        \n",
    "        flat_size = int((input_shape[-1] - 2 * 2) / 2 * 32 * input_shape[-2])\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(flat_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_cam(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN_GradCAM(X_ts_train.shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.train()\n",
    "    \n",
    "    for idx, (inputs, labels) in tqdm(enumerate(data_loader), total=data_loader.__len__(), disable=True):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs.float())[:, 0]\n",
    "        \n",
    "        loss = loss_function(preds ,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        \n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    train_loss = train_loss.detach().numpy()\n",
    "    return train_loss\n",
    "\n",
    "def validate(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs.float())[:, 0]\n",
    "            loss = loss_function(preds,labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "        valid_loss = running_loss/len(data_loader)\n",
    "        valid_loss = valid_loss.detach().numpy()\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "epochs = 10\n",
    "print(\"Started learning for %i epochs...\" % epochs)\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss = fit(model, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_loss = validate(model, test_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "    time_elapsed = time.time() - start\n",
    "    \n",
    "    print('Epochs %i/%i (%.3g seconds)\\n    Train loss = %.3g \\n    Valid loss = %.3g' % (epoch+1, epochs, time_elapsed, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "epochs_passed = len(train_losses)\n",
    "ax.plot(range(1, epochs_passed + 1), train_losses, label=\"Train\", linewidth=2)\n",
    "ax.plot(range(1, epochs_passed + 1), valid_losses, label=\"Validation\", linewidth=2)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlim(1, len(train_losses))\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_yticks([1, 10, 100])\n",
    "ax.grid(axis='y', alpha=0.5)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.forward(X_ts_test[::100]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(y_ts_test[::10], y_test_pred, s=1)\n",
    "ax.plot([-15, 35], [-15, 35], linewidth=1, color=\"black\", linestyle=\"--\")\n",
    "ax.set_xlim(-15, 35)\n",
    "ax.set_ylim(-15, 35)\n",
    "ax.set_xlabel(\"Target values\")\n",
    "ax.set_ylabel(\"Predicted values\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(time_ts_test[::100], y_ts_test[::100], label=\"Target\")\n",
    "ax.plot(time_ts_test[::100], y_test_pred, label=\"Prediction\")\n",
    "ax.set_ylabel(\"Temperature [C]\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(heatmap, std_vmin=False, use_abs=False):\n",
    "    \"\"\"\n",
    "    Function to plot explanation heatmap\n",
    "    \"\"\"\n",
    "    if use_abs:\n",
    "        heatmap = abs(heatmap)\n",
    "    \n",
    "    vmax = heatmap.mean() + 3 * heatmap.std()\n",
    "    if std_vmin:\n",
    "        vmin = heatmap.mean() - 3 * heatmap().std()\n",
    "    else:\n",
    "        vmin=0\n",
    "        \n",
    "    t = heatmap.shape[-1] -1 # time \n",
    "    \n",
    "    fig, ax =plt.subplots(figsize=(8, 4))\n",
    "    ax.imshow(heatmap.squeeze(), aspect=2, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xlabel(\"Time back [h]\")\n",
    "    ax.set_yticks(range(12))\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_xticks(np.linspace(0, t, 8).astype(int))\n",
    "    ax.set_xticklabels(np.linspace(t, 0, 8).astype(int))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/jacobgil/pytorch-grad-cam/issues/233\n",
    "# https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "# https://arxiv.org/pdf/2001.07582.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genereate_gradcam_explanation(model, x):\n",
    "    pred = model.forward_cam(x)\n",
    "    pred.backward()\n",
    "    gradients = model.get_activations_gradient()\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 1, 2])\n",
    "    activations = model.get_activations(x).detach()\n",
    "\n",
    "    for i in range(activations.shape[-1]):\n",
    "        activations[:, :, :, i] *= pooled_gradients[i]\n",
    "\n",
    "    mean_activations = torch.mean(activations, dim=[0, 1]).squeeze()\n",
    "    mean_activations = np.maximum(mean_activations, 0)\n",
    "    mean_activations = reshape_transform(mean_activations, (x.shape[-2], x.shape[-1]))\n",
    "    \n",
    "    return mean_activations\n",
    "\n",
    "def reshape_transform(tensor, target_size):\n",
    "    tensor = tensor.reshape((1, 1, tensor.shape[0], tensor.shape[1]))\n",
    "    image_with_single_row = tensor[:, None, :, :]\n",
    "    # Lets make the time series into an image with 16 rows for easier visualization on screen later\n",
    "    return torch.nn.functional.interpolate(tensor, target_size, mode='bilinear')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of data for explanations\n",
    "x_batch, y_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x = x_batch[i:i+1]\n",
    "y = y_batch[i:i+1]\n",
    "\n",
    "explanation = genereate_gradcam_explanation(model, x)\n",
    "plot_heatmap(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import GradientExplainer\n",
    "grad_explainer = GradientExplainer(model, X_ts_test)\n",
    "x_shap_values = grad_explainer.shap_values(x)\n",
    "\n",
    "plot_heatmap(x_shap_values, use_abs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
