{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with CNN and Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import shap\n",
    "\n",
    "colormap = ListedColormap([\"#ff595e\",\"#ffca3a\",\"#8ac926\",\"#52a675\",\"#1982c4\",\"#6a4c93\"], name=\"Custom\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colormap.colors)\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"jena_climate_2009_2016.csv\"\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "download_and_extract_archive(url=uri, download_root=os.getcwd(), filename=csv_filename+\".zip\")\n",
    "\n",
    "df = pd.read_csv(csv_filename)\n",
    "# convert 'Date Time' to datetime object\n",
    "df['Date Time'] = df['Date Time'].apply(lambda x: datetime.strptime(x, \"%d.%m.%Y %H:%M:%S\"))\n",
    "\n",
    "# convert angle to cos & sin\n",
    "df[\"cos(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.cos(x / 360 * 2 * np.pi))\n",
    "df[\"sin(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.sin(x / 360 * 2 * np.pi))\n",
    "\n",
    "df = df.drop([\"wd (deg)\"], axis=1)\n",
    "\n",
    "features = df.columns.values[1:]\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers\n",
    "\n",
    "remove_outliers = True\n",
    "plot_on_timeseries = True\n",
    "\n",
    "if plot_on_timeseries:\n",
    "    fig, ax = plt.subplots(figsize=(16, 3))\n",
    "\n",
    "for i, X in enumerate(features):\n",
    "    q05 = df[X].quantile(0.05)\n",
    "    q95 = df[X].quantile(0.95)\n",
    "    iqr = df[X].quantile(0.75) - df[X].quantile(0.25)\n",
    "    \n",
    "    lower_bound = q05 - 3 * iqr\n",
    "    upper_bound = q95 + 3 * iqr\n",
    "    \n",
    "    outliers = ((df[X] < lower_bound) | (df[X] > upper_bound))\n",
    "\n",
    "    if outliers.sum() > 0:\n",
    "        print(\"%s has %i outliers (lower=%.4g, upper=%.4g)\" % (X, outliers.sum(), lower_bound, upper_bound))\n",
    "    \n",
    "    if plot_on_timeseries:\n",
    "        label = X\n",
    "        for obs in df.loc[outliers, \"Date Time\"]:\n",
    "            ax.axvline(x=obs, color=colormap(i), label=label)\n",
    "            label=None\n",
    "    \n",
    "    if remove_outliers:\n",
    "        df = df[~outliers]\n",
    "        \n",
    "\n",
    "if plot_on_timeseries:\n",
    "    ax.set_title(\"Outliers\")\n",
    "    ax.legend(bbox_to_anchor=(0.5, 1.2), loc='center', ncol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[features].corr()\n",
    "redundant_features = []\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        col_i = features[i]\n",
    "        col_j = features[j]\n",
    "        \n",
    "        if correlation_matrix.loc[col_i, col_j] > 0.98:\n",
    "            print(\"%s and %s are redundant\" % (col_i, col_j))\n",
    "            redundant_features.append(col_j)\n",
    "\n",
    "df = df.drop(redundant_features, axis=1, errors='ignore')\n",
    "\n",
    "df = df.set_index(\"Date Time\").resample(\"H\").mean().reset_index()\n",
    "\n",
    "features = df.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 4\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    ax.hist(df[X], bins=50)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 2\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "sample_low = 0\n",
    "sample_high = 52560 * 2 # 2 years\n",
    "sample_high = -1\n",
    "skip = 6\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    plot_x = df[\"Date Time\"].iloc[sample_low:sample_high:skip]\n",
    "    plot_y = df[X].iloc[sample_low:sample_high:skip]\n",
    "    ax.plot(plot_x, plot_y, linewidth=1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "        if i<=j:\n",
    "            correlation_matrix.iloc[i, j] = np.nan\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(correlation_matrix, cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "plt.colorbar(label=\"Correlation coefficient\", location=\"top\", shrink=0.6)\n",
    "plt.xticks(ticks=range(len(features)), labels=features, rotation=90)\n",
    "plt.yticks(ticks=range(len(features)), labels=features, rotation=0)\n",
    "plt.xlim(-0.5, len(features) - 1.5 )\n",
    "plt.ylim(len(features)-0.5, 0.5 )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split, scale & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_future = 12\n",
    "hours_past = 48\n",
    "\n",
    "time_array = df['Date Time']\n",
    "X = df[features]\n",
    "y = df['T (degC)'].shift(-(hours_future + hours_past))\n",
    "\n",
    "# split into test and train datasets\n",
    "time_train, time_test, X_train, X_test, y_train, y_test = train_test_split(time_array, X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# scale datasets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch from pandas Series to numpy array\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(X, sequence_length=48):\n",
    "    X_seq = []\n",
    "    for i in range(sequence_length):\n",
    "        X_roll = X.roll(-i, dims=0)\n",
    "        X_seq.append(X_roll)\n",
    "    \n",
    "    X_seq = torch.stack(X_seq, dim=2)\n",
    "    \n",
    "    # expand the dimensions\n",
    "    X_seq = X_seq[:, None, :, :]\n",
    "    \n",
    "    return X_seq\n",
    "\n",
    "\n",
    "def prepare_data(X, y, time_array, hours_past=48, hours_future=12, batch_size=32):\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    \n",
    "    X_ts = prepare_sequences(X, hours_past)\n",
    "    X_ts = X_ts[:-(hours_past + hours_future)]\n",
    "    y_ts = y[:-(hours_past + hours_future)]\n",
    "        \n",
    "    time_array = time_array[:-(hours_past + hours_future)]\n",
    "    \n",
    "    # we verify which observations are valid i.e. there is proper time difference between the observations\n",
    "    # valid_observations = ((time_array.diff(-hours_past).dt.total_seconds() / 3600 ) ==  -hours_past ) & ((time_array.diff(hours_future).dt.total_seconds() / 3600 ) ==  hours_future )\n",
    "    \n",
    "    x_nan_observations = torch.isnan(X_ts).any(dim=1).any(dim=1).any(dim=1)\n",
    "    y_nan_observations = torch.isnan(y_ts)\n",
    "    \n",
    "    nan_observations = x_nan_observations | y_nan_observations\n",
    "    \n",
    "    time_ts = time_array[~nan_observations.numpy()]\n",
    "    X_ts = X_ts[~nan_observations]\n",
    "    y_ts = y_ts[~nan_observations]\n",
    "    \n",
    "    return time_ts, X_ts, y_ts\n",
    "\n",
    "time_ts_train, X_ts_train, y_ts_train = prepare_data(X_train, y_train, time_train)\n",
    "time_ts_test, X_ts_test, y_ts_test = prepare_data(X_test, y_test, time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self,features, target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.features[idx]\n",
    "        label = self.target[idx]\n",
    "        \n",
    "        return item,label\n",
    "    \n",
    "train = WeatherDataset(X_ts_train, y_ts_train)\n",
    "test = WeatherDataset(X_ts_test, y_ts_test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GradCAM(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 3))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2))\n",
    "        \n",
    "        flat_size = int((input_shape[-1] - 4 * 2) / 4 * 32 * input_shape[-2])\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(flat_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1st convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 2nd convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten & fully connected layers\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward_cam(self, x):\n",
    "        # 1st convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 2nd convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten & fully connected layers\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN_GradCAM(X_ts_train.shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.train()\n",
    "    \n",
    "    for idx, (inputs, labels) in tqdm(enumerate(data_loader), total=data_loader.__len__(), disable=True):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs.float())[:, 0]\n",
    "        \n",
    "        loss = loss_function(preds ,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        \n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    train_loss = train_loss.detach().numpy()\n",
    "    return train_loss\n",
    "\n",
    "def validate(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs.float())[:, 0]\n",
    "            loss = loss_function(preds,labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "        valid_loss = running_loss/len(data_loader)\n",
    "        valid_loss = valid_loss.detach().numpy()\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "epochs = 10\n",
    "print(\"Started learning for %i epochs...\" % epochs)\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss = fit(model, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_loss = validate(model, test_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "    time_elapsed = time.time() - start\n",
    "    \n",
    "    print('Epochs %i/%i (%.3g seconds)\\n    Train loss = %.3g \\n    Valid loss = %.3g' % (epoch+1, epochs, time_elapsed, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "epochs_passed = len(train_losses)\n",
    "ax.plot(range(1, epochs_passed + 1), train_losses, label=\"Train\", linewidth=2, marker='o', markersize=4)\n",
    "ax.plot(range(1, epochs_passed + 1), valid_losses, label=\"Validation\", linewidth=2, marker='o', markersize=4)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlim(1, len(train_losses))\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.5)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y, y_pred):\n",
    "    ss_tot = torch.sum((y - torch.mean(y)) ** 2)\n",
    "    ss_res = torch.sum((y - y_pred) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "time_ts_test_sample = time_ts_test[::5]\n",
    "X_ts_test_sample = X_ts_test[::5]\n",
    "y_ts_test_sample = y_ts_test[::5]\n",
    "\n",
    "y_test_pred = model.forward(X_ts_test_sample).detach().view(-1)\n",
    "\n",
    "print(\"R2 score: %.3f\" % r2_score(y_ts_test_sample, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(y_ts_test_sample, y_test_pred, s=5, alpha=0.3)\n",
    "ax.plot([-15, 35], [-15, 35], linewidth=1, color=\"black\", linestyle=\"--\")\n",
    "ax.set_xlim(-15, 35)\n",
    "ax.set_ylim(-15, 35)\n",
    "ax.set_xlabel(\"Target values\")\n",
    "ax.set_ylabel(\"Predicted values\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(time_ts_test_sample, y_ts_test_sample, label=\"Target\")\n",
    "ax.plot(time_ts_test_sample, y_test_pred, label=\"Prediction\", alpha=1, linewidth=1.5)\n",
    "ax.set_ylabel(\"Temperature [C]\")\n",
    "ax.legend()\n",
    "ax.set_xlim(time_ts_test_sample.values[-1000], time_ts_test_sample.values[-700])\n",
    "ax.set_yticks(np.arange(-15, 35.1, 5))\n",
    "ax.grid(axis='y')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/jacobgil/pytorch-grad-cam/issues/233\n",
    "# https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "# https://arxiv.org/pdf/2001.07582.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, target_size):\n",
    "    \"\"\" \n",
    "    Transforms a tensor to the required shape by interpolation\n",
    "    Used to transform the tensor before last pooling layer to input size.\n",
    "    Note: this should only interpolate time axis, not the feature axis!\n",
    "    \"\"\"\n",
    "    tensor = tensor.reshape((1, 1, tensor.shape[0], tensor.shape[1]))\n",
    "    image_with_single_row = tensor[:, None, :, :]\n",
    "    # Lets make the time series into an image with 16 rows for easier visualization on screen later\n",
    "    return torch.nn.functional.interpolate(tensor, target_size, mode='bilinear')\n",
    "\n",
    "def genereate_gradcam_explanation(model, x):\n",
    "    pred = model.forward_cam(x)\n",
    "    pred.backward()\n",
    "    gradients = model.get_activations_gradient()\n",
    "    print(\"Gradients shape: {}\".format(gradients.shape))\n",
    "    \n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    \n",
    "    print(\"Pooled Gradients shape: {}\".format(pooled_gradients.shape))\n",
    "    \n",
    "    activations = model.get_activations(x).detach()\n",
    "    print(\"Activations shape: {}\".format(activations.shape))\n",
    "\n",
    "    for i in range(activations.shape[1]):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "    mean_activations = torch.mean(activations, dim=1).squeeze()\n",
    "    mean_activations = torch.maximum(mean_activations, torch.tensor(0))\n",
    "    mean_activations = reshape_transform(mean_activations, (x.shape[-2], x.shape[-1]))\n",
    "    return mean_activations\n",
    "\n",
    "\n",
    "def plot_explanation_heatmap(heatmap, std_vmin=False, use_abs=False):\n",
    "    \"\"\"\n",
    "    Function to plot explanation heatmap\n",
    "    \"\"\"\n",
    "    if use_abs:\n",
    "        heatmap = abs(heatmap)\n",
    "    \n",
    "    vmax = heatmap.mean() + 3 * heatmap.std()\n",
    "    if std_vmin:\n",
    "        vmin = heatmap.mean() - 3 * heatmap().std()\n",
    "    else:\n",
    "        vmin=0\n",
    "        \n",
    "    t = heatmap.shape[-1] -1 # time \n",
    "    \n",
    "    fig, ax =plt.subplots(figsize=(8, 4))\n",
    "    ax.imshow(heatmap.squeeze(), aspect=2, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xlabel(\"Time back [h]\")\n",
    "    ax.set_yticks(range(12))\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_xticks(np.linspace(0, t, 8).astype(int))\n",
    "    ax.set_xticklabels(np.linspace(t, 0, 8).astype(int))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of data for explanations\n",
    "x_batch, y_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30 # select one observation from batch\n",
    "x = x_batch[i:i+1]\n",
    "y = y_batch[i:i+1]\n",
    "\n",
    "explanation = genereate_gradcam_explanation(model, x)\n",
    "plot_explanation_heatmap(explanation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
