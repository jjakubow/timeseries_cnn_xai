{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with CNN and Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import shap\n",
    "\n",
    "colormap = ListedColormap([\"#ff595e\",\"#ffca3a\",\"#8ac926\",\"#52a675\",\"#1982c4\",\"#6a4c93\"], name=\"Custom\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colormap.colors)\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"jena_climate_2009_2016.csv\"\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "download_and_extract_archive(url=uri, download_root=os.getcwd(), filename=csv_filename+\".zip\")\n",
    "\n",
    "df = pd.read_csv(csv_filename)\n",
    "# convert 'Date Time' to datetime object\n",
    "df['Date Time'] = df['Date Time'].apply(lambda x: datetime.strptime(x, \"%d.%m.%Y %H:%M:%S\"))\n",
    "\n",
    "# convert angle to cos & sin\n",
    "df[\"cos(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.cos(x / 360 * 2 * np.pi))\n",
    "df[\"sin(wd)\"] = df[\"wd (deg)\"].apply(lambda x: np.sin(x / 360 * 2 * np.pi))\n",
    "\n",
    "df = df.drop([\"wd (deg)\"], axis=1)\n",
    "\n",
    "features = df.columns.values[1:]\n",
    "\n",
    "# df = df.iloc[:20000]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers\n",
    "\n",
    "remove_outliers = True\n",
    "plot_on_timeseries = True\n",
    "\n",
    "if plot_on_timeseries:\n",
    "    fig, ax = plt.subplots(figsize=(16, 3))\n",
    "\n",
    "for i, X in enumerate(features):\n",
    "    q05 = df[X].quantile(0.05)\n",
    "    q95 = df[X].quantile(0.95)\n",
    "    iqr = df[X].quantile(0.75) - df[X].quantile(0.25)\n",
    "    \n",
    "    lower_bound = q05 - 3 * iqr\n",
    "    upper_bound = q95 + 3 * iqr\n",
    "    \n",
    "    outliers = ((df[X] < lower_bound) | (df[X] > upper_bound))\n",
    "\n",
    "    if outliers.sum() > 0:\n",
    "        print(\"%s has %i outliers (lower=%.4g, upper=%.4g)\" % (X, outliers.sum(), lower_bound, upper_bound))\n",
    "    \n",
    "    if plot_on_timeseries:\n",
    "        label = X\n",
    "        for obs in df.loc[outliers, \"Date Time\"]:\n",
    "            ax.axvline(x=obs, color=colormap(i), label=label)\n",
    "            label=None\n",
    "    \n",
    "    if remove_outliers:\n",
    "        df = df[~outliers]\n",
    "        \n",
    "\n",
    "if plot_on_timeseries:\n",
    "    ax.set_title(\"Outliers\")\n",
    "    ax.legend(bbox_to_anchor=(0.5, 1.2), loc='center', ncol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[features].corr()\n",
    "redundant_features = []\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        col_i = features[i]\n",
    "        col_j = features[j]\n",
    "        \n",
    "        if correlation_matrix.loc[col_i, col_j] > 0.98:\n",
    "            print(\"%s and %s are redundant\" % (col_i, col_j))\n",
    "            redundant_features.append(col_j)\n",
    "\n",
    "df = df.drop(redundant_features, axis=1, errors='ignore')\n",
    "\n",
    "features = df.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 4\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    ax.hist(df[X], bins=50)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = 2\n",
    "plot_rows = len(features) // plot_cols + min(len(features) % plot_cols, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 3 * plot_rows), nrows=plot_rows, ncols=plot_cols)\n",
    "\n",
    "sample_low = 0\n",
    "sample_high = 52560 * 2 # 2 years\n",
    "sample_high = -1\n",
    "skip = 6\n",
    "\n",
    "for ax, X in zip(axes.flatten(), features):\n",
    "    ax.set_title(X)\n",
    "    plot_x = df[\"Date Time\"].iloc[sample_low:sample_high:skip]\n",
    "    plot_y = df[X].iloc[sample_low:sample_high:skip]\n",
    "    ax.plot(plot_x, plot_y, linewidth=1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split, scale & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_future = 12\n",
    "hours_past = 168\n",
    "skip = 6\n",
    "\n",
    "time_array = df['Date Time'].iloc[:-(hours_future + hours_past)*6:skip]\n",
    "X = df[features].iloc[:-(hours_future + hours_past)*6:skip]\n",
    "y = df['T (degC)'].shift(-(hours_future + hours_past)*6).iloc[:-(hours_future + hours_past)*6:skip]\n",
    "\n",
    "# split into test and train datasets\n",
    "time_train, time_test, X_train, X_test, y_train, y_test = train_test_split(time_array, X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# scale datasets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch from pandas Series to numpy array\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(X, sequence_length=168):\n",
    "    X_seq = []\n",
    "    for i in range(sequence_length):\n",
    "        X_roll = X.roll(-i, dims=0)\n",
    "        X_seq.append(X_roll)\n",
    "    \n",
    "    X_seq = torch.stack(X_seq, dim=2)\n",
    "    X_seq = X_seq[:-sequence_length]\n",
    "    \n",
    "    return X_seq\n",
    "\n",
    "\n",
    "def prepare_data(X, y, time_array, hours_past=168, hours_future=12, batch_size=32):\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    \n",
    "    X_ts = prepare_sequences(X, hours_past)\n",
    "    y_ts = y[:-hours_past]\n",
    "    time_array = time_array[:-hours_past]\n",
    "    \n",
    "    # we verify which observations are valid i.e. there is proper time difference between the observations\n",
    "    valid_observations = ((time_array.diff(-hours_past).dt.total_seconds() / 3600 ) ==  -hours_past ) & ((time_array.diff(hours_future).dt.total_seconds() / 3600 ) ==  hours_future )\n",
    "    \n",
    "    time_ts = time_array[valid_observations]\n",
    "    X_ts = X_ts[valid_observations.values]\n",
    "    y_ts = y_ts[valid_observations.values]\n",
    "    \n",
    "    return time_ts, X_ts, y_ts\n",
    "\n",
    "time_ts_train, X_ts_train, y_ts_train = prepare_data(X_train, y_train, time_train)\n",
    "time_ts_test, X_ts_test, y_ts_test = prepare_data(X_test, y_test, time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self,features, target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.features[idx]\n",
    "        label = self.target[idx]\n",
    "        \n",
    "        return item,label\n",
    "    \n",
    "train = WeatherDataset(X_ts_train, y_ts_train)\n",
    "test = WeatherDataset(X_ts_test, y_ts_test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, 32, kernel_size=3), # 32 x 166\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3), # 64 x 164\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2), # 64 x 82\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(64*82, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_1 = nn.Conv1d(n_features, 32, kernel_size=3)\n",
    "        self.conv1d_2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1d = nn.MaxPool1d(2)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    \n",
    "device = torch.device(\"cpu\")\n",
    "model = CNN(len(features)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.train()\n",
    "    \n",
    "    for idx, (inputs, labels) in tqdm(enumerate(data_loader), total=data_loader.__len__(), disable=True):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs.float())[:, 0]\n",
    "        \n",
    "        loss = loss_function(preds ,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        \n",
    "    train_loss = running_loss/len(data_loader)\n",
    "    train_loss = train_loss.detach().numpy()\n",
    "    return train_loss\n",
    "\n",
    "def validate(model, data_loader):\n",
    "    running_loss = .0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs.float())[:, 0]\n",
    "            loss = loss_function(preds,labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "        valid_loss = running_loss/len(data_loader)\n",
    "        valid_loss = valid_loss.detach().numpy()\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss = fit(model, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_loss = validate(model, test_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "    time_elapsed = time.time() - start\n",
    "    \n",
    "    print('Epochs %i/%i (%.3g seconds)\\n    Train loss = %.3g \\n    Valid loss = %.3g' % (epoch+1, epochs, time_elapsed, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "epochs_passed = len(train_losses)\n",
    "ax.plot(range(1, epochs_passed + 1), train_losses, label=\"Train\", linewidth=2)\n",
    "ax.plot(range(1, epochs_passed + 1), valid_losses, label=\"Validation\", linewidth=2)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlim(1, len(train_losses))\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_yticks([1, 10, 100])\n",
    "ax.grid(axis='y', alpha=0.5)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.forward(X_ts_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(y_ts_test, y_test_pred, s=1)\n",
    "ax.plot([-15, 35], [-15, 35], linewidth=1, color=\"black\", linestyle=\"--\")\n",
    "ax.set_xlim(-15, 35)\n",
    "ax.set_ylim(-15, 35)\n",
    "ax.set_xlabel(\"Target values\")\n",
    "ax.set_ylabel(\"Predicted values\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(time_ts_test[-600:], y_ts_test[-600:], label=\"Target\")\n",
    "ax.plot(time_ts_test[-600:], y_test_pred[-600:], label=\"Prediction\")\n",
    "ax.set_ylabel(\"Temperature [C]\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of data for explanations\n",
    "\n",
    "x_batch, y_batch = next(iter(test_loader))\n",
    "x = x_batch[0:1]\n",
    "y = y_batch[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# explainer = shap.GradientExplainer((model, model.network[0]), X_ts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/jacobgil/pytorch-grad-cam/issues/233\n",
    "# https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "# https://arxiv.org/pdf/2001.07582.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GradCAM(nn.Module):\n",
    "    def __init__(self, model, last_pooling_idx):\n",
    "        super(CNN_GradCAM, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.detached_model = self.model.network[:last_pooling_idx]\n",
    "        self.last_pool = self.model.network[last_pooling_idx]\n",
    "        self.fc = self.model.network[last_pooling_idx+1:]\n",
    "        self.gradients = None\n",
    "        \n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.detached_model(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.last_pool(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_activations_gradients(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activation(self, x):\n",
    "        return self.detached_model(x)\n",
    "    \n",
    "\n",
    "cam = CNN_GradCAM(model, 4)\n",
    "cam.eval()\n",
    "\n",
    "pred = cam(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = cam.get_activations_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradients = torch.mean(gradients, dim=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(164):\n",
    "    activations[:, :, i] *= pooled_gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(gradients, dim=[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = cam.get_activation(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = torch.mean(activations, dim=2).squeeze()\n",
    "heatmap = np.maximum(heatmap, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
